# Reference Image Integration Plan for falLoRA

## Project Overview

This document outlines the comprehensive plan for integrating reference image-based LoRA generation into the existing falLoRA application. The integration will allow users to upload a reference image, generate character LoRAs that match the reference while incorporating user-specified physical attributes.

## üéØ CRITICAL UPDATE: Simplified Architecture

**Discovery**: fal.ai FLUX Pro depth ControlNet processes the original reference image internally to generate depth maps. **No local depth processing required!**

This dramatically simplifies the implementation by eliminating:
- ‚ùå Depth-Anything-V2 integration
- ‚ùå CPU-intensive depth map generation
- ‚ùå Heavy ML dependencies (torch, opencv, transformers)
- ‚ùå 30-60 second processing delays
- ‚ùå Large Docker container size increases

## Current Architecture Analysis

### Existing System
- **Backend**: Python Flask with async job processing
- **Frontend**: HTML/CSS/JavaScript with Tailwind CSS and Alpine.js
- **API Integration**: fal.ai endpoints for image generation
- **Infrastructure**: Docker containerized application
- **Current Models**: FLUX, FLUX-Kontext, WAN v2.2, Qwen with Civitai LoRA support

### Current API Endpoints
```python
FAL_ENDPOINTS = {
    "fal-ai/flux-lora": "https://fal.run/fal-ai/flux-lora",
    "fal-ai/flux-kontext-lora": "https://fal.run/fal-ai/flux-kontext-lora/text-to-image",
    "fal-ai/wan/v2.2-a14b/text-to-image/lora": "https://fal.run/fal-ai/wan/v2.2-a14b/text-to-image/lora",
    "fal-ai/qwen-image": "https://fal.run/fal-ai/qwen-image"
}
```

### New API Endpoint (Reference Mode)
```python
# Add for reference image generation with LoRA + depth control
"fal-ai/flux-control-lora-depth": "https://fal.run/fal-ai/flux-control-lora-depth/image-to-image"
```

## Integration Requirements

### Core Features
1. **Reference Image Upload**: Drag & drop interface for reference images
2. **Automatic Depth Processing**: fal.ai processes reference images internally (no local processing!)
3. **ControlNet Integration**: fal.ai FLUX Control LoRA Depth with automatic depth processing and LoRA support
4. **AI Prompt Generation**: z.ai GLM-4.5v for analyzing reference images
5. **Physical Attributes**: User-configurable character attributes
6. **Visual Toggle**: Enable/disable reference mode while keeping image loaded

### External Dependencies
- **fal.ai FLUX Control LoRA Depth API**: `https://fal.run/fal-ai/flux-control-lora-depth/image-to-image`
- **z.ai GLM-4.5v**: OpenAI-compatible vision model at `https://api.z.ai/api/paas/v4`

### Removed Dependencies (Thanks to Simplified Architecture!)
- ~~**Shakker-Labs ControlNet**: Not needed, fal.ai handles internally~~
- ~~**Depth-Anything-V2**: Not needed, fal.ai processes depth internally~~
- ~~**Heavy ML Dependencies**: torch, opencv, transformers not required~~

## Technical Architecture

### 1. Backend Modifications

#### New Environment Variables (`.env`)
```bash
# Existing
FAL_KEY=your_fal_key
CIVITAI_TOKEN=your_civitai_token

# New additions
Z_AI_API_KEY=your_z_ai_personal_access_token
REFERENCE_IMAGE_DIR=/tmp/fallora_uploads
MAX_REFERENCE_IMAGE_SIZE=10485760  # 10MB
```

#### New API Endpoints

**File Upload Endpoint**
```python
@app.route('/api/upload-reference', methods=['POST'])
def upload_reference_image():
    """Upload and store reference image"""
    # Validate file type and size
    # Save to temporary directory
    # Generate unique filename
    # Return file info and public access URL
```

**~~Depth Map Generation Endpoint~~ (REMOVED - Not Needed!)**
```python
# ‚ùå No longer needed - fal.ai handles depth processing internally
# @app.route('/api/generate-depth', methods=['POST'])
# def generate_depth_map():
#     """Generate depth map from reference image"""
```

**Prompt Analysis Endpoint**
```python
@app.route('/api/analyze-image', methods=['POST'])
def analyze_reference_image():
    """Analyze reference image with z.ai GLM-4.5v"""
    # Convert image to base64
    # Send to z.ai API
    # Extract descriptive prompt
    # Return suggested prompt text
```

**Enhanced Generation Endpoint**
```python
# Modify existing /api/generate endpoint
def submit_generation_job():
    # Add support for reference mode
    # Handle FLUX Pro depth ControlNet configuration
    # Pass reference image URL directly to fal.ai
    # Process physical attributes and enhance prompt
```

#### New Dependencies (`requirements.txt`)
```txt
# Existing dependencies
flask
python-dotenv
huggingface_hub
flask-cors
requests

# New dependencies for reference image processing (SIMPLIFIED!)
pillow  # For basic image handling only
```

### 2. Frontend Modifications

#### New UI Components

**Reference Image Section**
```html
<!-- New section in main form -->
<div class="form-group" id="reference-section">
  <div class="flex items-center justify-between mb-2">
    <label class="block text-sm font-medium text-gray-300">üñºÔ∏è Reference Image</label>
    <div class="flex items-center gap-2">
      <span class="text-sm text-gray-400">Reference Mode</span>
      <input type="checkbox" id="reference-toggle" class="toggle-switch">
    </div>
  </div>

  <!-- Upload Area -->
  <div id="upload-area" class="border-2 border-dashed border-gray-600 rounded-lg p-6 text-center">
    <input type="file" id="reference-upload" accept="image/*" class="hidden">
    <div class="upload-content">
      <svg class="mx-auto h-12 w-12 text-gray-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12" />
      </svg>
      <p class="mt-2 text-sm text-gray-400">Drop reference image here or click to upload</p>
      <p class="text-xs text-gray-500">PNG, JPG up to 10MB</p>
    </div>
  </div>

  <!-- Image Preview -->
  <div id="reference-preview" class="hidden mt-4">
    <div class="grid grid-cols-1 gap-4">
      <div>
        <p class="text-sm text-gray-400 mb-2">Reference Image</p>
        <img id="reference-image" class="w-full rounded-lg">
        <p class="text-xs text-green-400 mt-2">‚úÖ fal.ai will automatically process depth information from this image</p>
      </div>
    </div>
  </div>
</div>
```

**Physical Attributes Section**
```html
<div class="form-group" id="attributes-section" style="display: none;">
  <label class="block text-sm font-medium text-gray-300 mb-4">üë§ Physical Attributes</label>

  <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
    <div>
      <label for="hair-color" class="block text-xs text-gray-400 mb-1">Hair Color</label>
      <select id="hair-color" class="w-full bg-gray-700 border-gray-600 rounded-md py-2 px-3 text-sm">
        <option value="">Any</option>
        <option value="blonde">Blonde</option>
        <option value="brunette">Brunette</option>
        <option value="black">Black</option>
        <option value="red">Red</option>
        <option value="auburn">Auburn</option>
        <option value="gray">Gray</option>
        <option value="white">White</option>
        <option value="colorful">Colorful</option>
      </select>
    </div>

    <div>
      <label for="hair-style" class="block text-xs text-gray-400 mb-1">Hair Style</label>
      <select id="hair-style" class="w-full bg-gray-700 border-gray-600 rounded-md py-2 px-3 text-sm">
        <option value="">Any</option>
        <option value="long">Long</option>
        <option value="short">Short</option>
        <option value="medium">Medium</option>
        <option value="curly">Curly</option>
        <option value="straight">Straight</option>
        <option value="wavy">Wavy</option>
        <option value="pixie">Pixie Cut</option>
        <option value="bob">Bob</option>
        <option value="ponytail">Ponytail</option>
        <option value="braided">Braided</option>
      </select>
    </div>

    <div>
      <label for="eye-color" class="block text-xs text-gray-400 mb-1">Eye Color</label>
      <select id="eye-color" class="w-full bg-gray-700 border-gray-600 rounded-md py-2 px-3 text-sm">
        <option value="">Any</option>
        <option value="brown">Brown</option>
        <option value="blue">Blue</option>
        <option value="green">Green</option>
        <option value="hazel">Hazel</option>
        <option value="gray">Gray</option>
        <option value="amber">Amber</option>
        <option value="violet">Violet</option>
      </select>
    </div>
  </div>
</div>
```

**AI Prompt Analysis Section**
```html
<div class="form-group" id="ai-analysis-section" style="display: none;">
  <div class="flex items-center justify-between mb-2">
    <label class="block text-sm font-medium text-gray-300">ü§ñ AI Prompt Analysis</label>
    <button type="button" id="analyze-image" class="bg-purple-600 hover:bg-purple-700 text-white py-1 px-3 rounded text-sm">
      Analyze Reference
    </button>
  </div>
  <textarea id="ai-suggested-prompt" rows="3" class="w-full bg-gray-700 border-gray-600 rounded-md py-2 px-3 text-sm"
            placeholder="AI-generated description will appear here..." readonly></textarea>
  <button type="button" id="use-ai-prompt" class="mt-2 text-sm text-indigo-400 hover:text-indigo-300">
    Use AI Suggestion in Prompt
  </button>
</div>
```

#### JavaScript Enhancements

**File Upload Handling**
```javascript
// Reference image upload with drag & drop
function initializeReferenceUpload() {
    const uploadArea = document.getElementById('upload-area');
    const fileInput = document.getElementById('reference-upload');

    // Drag & drop functionality
    uploadArea.addEventListener('dragover', (e) => {
        e.preventDefault();
        uploadArea.classList.add('border-indigo-500');
    });

    uploadArea.addEventListener('drop', (e) => {
        e.preventDefault();
        const files = e.dataTransfer.files;
        if (files.length > 0) {
            handleReferenceUpload(files[0]);
        }
    });

    // Click to upload
    uploadArea.addEventListener('click', () => fileInput.click());
    fileInput.addEventListener('change', (e) => {
        if (e.target.files.length > 0) {
            handleReferenceUpload(e.target.files[0]);
        }
    });
}

async function handleReferenceUpload(file) {
    // Validate file type and size
    if (!file.type.startsWith('image/')) {
        showAlert('Please upload an image file', 'error');
        return;
    }

    if (file.size > 10 * 1024 * 1024) { // 10MB
        showAlert('Image must be smaller than 10MB', 'error');
        return;
    }

    // Upload to server
    const formData = new FormData();
    formData.append('reference_image', file);

    try {
        const response = await fetch('/api/upload-reference', {
            method: 'POST',
            body: formData
        });

        const result = await response.json();
        if (response.ok) {
            displayReferenceImage(result.image_url);
            enableReferenceMode();
        } else {
            showAlert(result.error, 'error');
        }
    } catch (error) {
        showAlert('Upload failed: ' + error.message, 'error');
    }
}
```

**~~Depth Map Generation~~ (REMOVED - Not Needed!)**
```javascript
// ‚ùå No longer needed - fal.ai handles depth processing internally
// async function generateDepthMap(imageUrl) {
//     // Depth processing now handled by fal.ai
// }
```

**AI Prompt Analysis**
```javascript
async function analyzeReferenceImage(imageUrl) {
    const analyzeButton = document.getElementById('analyze-image');
    analyzeButton.disabled = true;
    analyzeButton.textContent = 'Analyzing...';

    try {
        const response = await fetch('/api/analyze-image', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ image_url: imageUrl })
        });

        const result = await response.json();
        if (response.ok) {
            document.getElementById('ai-suggested-prompt').value = result.suggested_prompt;
        } else {
            showAlert(result.error, 'error');
        }
    } catch (error) {
        showAlert('Analysis failed: ' + error.message, 'error');
    } finally {
        analyzeButton.disabled = false;
        analyzeButton.textContent = 'Analyze Reference';
    }
}
```

### 3. Docker Configuration Updates

#### Dockerfile Modifications (SIMPLIFIED!)
```dockerfile
FROM python:3.9-slim-buster

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Create upload directory (no depth map dir needed!)
RUN mkdir -p /tmp/fallora_uploads

# Copy application files
COPY . .

# Expose port
EXPOSE 5000

# Run the application
CMD ["python", "app.py"]
```

## Implementation Workflow

### Phase 1: Core Infrastructure (Week 1) - SIMPLIFIED!
1. **Environment Setup**
   - Add new environment variables
   - Update Docker configuration (minimal changes!)
   - Install only Pillow for basic image handling

2. **File Upload System**
   - Implement reference image upload endpoint
   - Add file validation and storage
   - Create temporary file management with public URL access

3. **Basic UI Framework**
   - Add reference image upload section
   - Implement drag & drop functionality
   - Create visual toggle for reference mode

### Phase 2: ControlNet Integration (Week 2) - MUCH FASTER!
1. **fal.ai FLUX Pro Depth Integration**
   - Add fal.ai FLUX Pro depth endpoint
   - Modify generation logic for ControlNet with reference images
   - Test direct reference image URL passing

2. **UI Enhancements**
   - Add reference image preview
   - Remove depth map generation UI (not needed!)
   - Add ControlNet status indicators

### Phase 2.5: Smart Cropping System (Week 2.5) - CRITICAL FOR CONTROLNET QUALITY

**Rationale**: When using depth ControlNet with reference images, aspect ratio mismatches between source reference and output resolution cause composition distortions. Users need interactive control over framing to ensure optimal results.

**User Workflow Design**:
1. **Upload Reference**: User uploads reference image
2. **Auto-Display**: Image automatically displays in output-sized window (default 1080x1080)
3. **Resolution Changes**: When output resolution changes, reference window resizes to match aspect ratio
4. **Interactive Cropping**: User can pan, zoom, and frame subject optimally
5. **Generate Cropped Version**: System creates optimally cropped version for fal.ai API

**Key Insight**: Focus on **aspect ratio and composition**, not absolute resolution. fal.ai handles final pixel scaling.

#### Technical Implementation

**Backend - Cropping API Endpoint**
```python
@app.route('/api/crop-reference', methods=['POST'])
def crop_reference_image():
    """Generate cropped version based on user framing"""
    data = request.json

    # Receive cropping parameters
    source_url = data['source_url']
    crop_params = {
        'x': data['offset_x'],  # Pan offset
        'y': data['offset_y'],  # Pan offset
        'scale': data['scale'],  # Zoom level
        'target_width': data['target_width'],
        'target_height': data['target_height']
    }

    # Use Pillow to crop and resize
    source_image = download_image(source_url)
    cropped_image = apply_crop_transform(source_image, crop_params)

    # Save cropped version for fal.ai API
    cropped_url = save_temp_image(cropped_image)

    return {'cropped_url': cropped_url}
```

**Frontend - Interactive Cropping Interface**
```html
<!-- Enhanced Reference Image Section with Cropping -->
<div class="form-group" id="reference-section">
  <div class="flex items-center justify-between mb-2">
    <label class="block text-sm font-medium text-gray-300">üñºÔ∏è Reference Image</label>
    <div class="flex items-center gap-2">
      <span class="text-sm text-gray-400">Reference Mode</span>
      <input type="checkbox" id="reference-toggle" class="toggle-switch">
    </div>
  </div>

  <!-- Upload Area (hidden when image loaded) -->
  <div id="upload-area" class="border-2 border-dashed border-gray-600 rounded-lg p-6 text-center">
    <!-- Upload UI -->
  </div>

  <!-- Reference Image with Cropping Interface -->
  <div id="reference-preview" class="hidden mt-4">
    <div class="grid grid-cols-1 gap-4">
      <!-- Cropping Container -->
      <div>
        <p class="text-sm text-gray-400 mb-2">
          Frame Reference Image
          <span id="output-dimensions" class="text-xs text-blue-400">(1080x1080)</span>
        </p>

        <!-- Interactive Cropping Window -->
        <div id="crop-container" class="relative overflow-hidden border-2 border-gray-600 rounded-lg bg-gray-800"
             style="width: 300px; height: 300px;">

          <!-- Reference Image (transformable) -->
          <img id="reference-image" class="absolute cursor-move"
               style="transform-origin: center; transition: transform 0.1s ease;">

          <!-- Crop Overlay -->
          <div class="absolute inset-0 pointer-events-none">
            <div class="grid grid-cols-3 grid-rows-3 h-full opacity-30">
              <!-- Rule of thirds grid -->
              <div class="border-r border-t border-gray-400"></div>
              <div class="border-r border-t border-gray-400"></div>
              <div class="border-t border-gray-400"></div>
              <div class="border-r border-t border-gray-400"></div>
              <div class="border-r border-t border-gray-400 border-dashed"></div>
              <div class="border-t border-gray-400"></div>
              <div class="border-r border-gray-400"></div>
              <div class="border-r border-gray-400"></div>
            </div>
          </div>
        </div>

        <!-- Cropping Controls -->
        <div class="flex items-center justify-between mt-2">
          <div class="flex items-center gap-2">
            <button id="zoom-out" class="bg-gray-700 hover:bg-gray-600 text-white w-8 h-8 rounded text-sm">‚àí</button>
            <span id="zoom-level" class="text-xs text-gray-400 w-12 text-center">100%</span>
            <button id="zoom-in" class="bg-gray-700 hover:bg-gray-600 text-white w-8 h-8 rounded text-sm">+</button>
          </div>

          <div class="flex items-center gap-2">
            <button id="reset-crop" class="bg-gray-700 hover:bg-gray-600 text-white text-xs px-2 py-1 rounded">Reset</button>
            <button id="apply-crop" class="bg-blue-600 hover:bg-blue-700 text-white text-xs px-2 py-1 rounded">Apply Crop</button>
          </div>
        </div>

        <p class="text-xs text-gray-500 mt-2">
          üí° Drag to move ‚Ä¢ Scroll wheel or +/- to zoom ‚Ä¢ Crop applies to output resolution
        </p>
      </div>
    </div>
  </div>
</div>
```

**JavaScript - Interactive Cropping Logic**
```javascript
class ReferenceImageCropper {
    constructor() {
        this.image = document.getElementById('reference-image');
        this.container = document.getElementById('crop-container');
        this.scale = 1.0;
        this.offsetX = 0;
        this.offsetY = 0;
        this.isDragging = false;
        this.lastMouseX = 0;
        this.lastMouseY = 0;

        this.initializeEventListeners();
        this.setupResolutionSync();
    }

    initializeEventListeners() {
        // Mouse events for panning
        this.image.addEventListener('mousedown', this.startDrag.bind(this));
        document.addEventListener('mousemove', this.drag.bind(this));
        document.addEventListener('mouseup', this.endDrag.bind(this));

        // Zoom controls
        document.getElementById('zoom-in').addEventListener('click', () => this.zoom(1.2));
        document.getElementById('zoom-out').addEventListener('click', () => this.zoom(0.8));

        // Mouse wheel zoom
        this.container.addEventListener('wheel', (e) => {
            e.preventDefault();
            const zoomFactor = e.deltaY > 0 ? 0.9 : 1.1;
            this.zoom(zoomFactor);
        });

        // Keyboard controls
        document.addEventListener('keydown', (e) => {
            if (e.target.tagName === 'INPUT') return;

            switch(e.key) {
                case '+':
                case '=':
                    this.zoom(1.2);
                    break;
                case '-':
                case '_':
                    this.zoom(0.8);
                    break;
                case 'ArrowUp':
                    this.offsetY += 20;
                    this.updateTransform();
                    break;
                case 'ArrowDown':
                    this.offsetY -= 20;
                    this.updateTransform();
                    break;
                case 'ArrowLeft':
                    this.offsetX += 20;
                    this.updateTransform();
                    break;
                case 'ArrowRight':
                    this.offsetX -= 20;
                    this.updateTransform();
                    break;
            }
        });

        // Crop controls
        document.getElementById('reset-crop').addEventListener('click', this.resetCrop.bind(this));
        document.getElementById('apply-crop').addEventListener('click', this.applyCrop.bind(this));
    }

    setupResolutionSync() {
        // Listen for output resolution changes
        const widthSelect = document.getElementById('width');
        const heightSelect = document.getElementById('height');

        [widthSelect, heightSelect].forEach(select => {
            select.addEventListener('change', () => {
                this.updateContainerSize();
            });
        });
    }

    updateContainerSize() {
        const width = parseInt(document.getElementById('width').value);
        const height = parseInt(document.getElementById('height').value);

        // Update container to match output aspect ratio
        const maxSize = 300; // Maximum display size
        const aspectRatio = width / height;

        if (aspectRatio > 1) {
            this.container.style.width = maxSize + 'px';
            this.container.style.height = (maxSize / aspectRatio) + 'px';
        } else {
            this.container.style.width = (maxSize * aspectRatio) + 'px';
            this.container.style.height = maxSize + 'px';
        }

        // Update dimension display
        document.getElementById('output-dimensions').textContent = `(${width}x${height})`;

        // Reset crop for new aspect ratio
        this.resetCrop();
    }

    startDrag(e) {
        this.isDragging = true;
        this.lastMouseX = e.clientX;
        this.lastMouseY = e.clientY;
        this.image.style.cursor = 'grabbing';
    }

    drag(e) {
        if (!this.isDragging) return;

        const deltaX = e.clientX - this.lastMouseX;
        const deltaY = e.clientY - this.lastMouseY;

        this.offsetX += deltaX;
        this.offsetY += deltaY;

        this.updateTransform();

        this.lastMouseX = e.clientX;
        this.lastMouseY = e.clientY;
    }

    endDrag() {
        this.isDragging = false;
        this.image.style.cursor = 'move';
    }

    zoom(factor) {
        this.scale = Math.max(0.1, Math.min(5.0, this.scale * factor));
        this.updateTransform();
        document.getElementById('zoom-level').textContent = Math.round(this.scale * 100) + '%';
    }

    updateTransform() {
        this.image.style.transform = `translate(${this.offsetX}px, ${this.offsetY}px) scale(${this.scale})`;
    }

    resetCrop() {
        this.scale = 1.0;
        this.offsetX = 0;
        this.offsetY = 0;
        this.updateTransform();
        document.getElementById('zoom-level').textContent = '100%';
    }

    async applyCrop() {
        const width = parseInt(document.getElementById('width').value);
        const height = parseInt(document.getElementById('height').value);

        try {
            const response = await fetch('/api/crop-reference', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    source_url: this.originalImageUrl,
                    offset_x: this.offsetX,
                    offset_y: this.offsetY,
                    scale: this.scale,
                    target_width: width,
                    target_height: height
                })
            });

            const result = await response.json();
            if (response.ok) {
                // Update reference image with cropped version
                this.croppedImageUrl = result.cropped_url;
                this.image.src = result.cropped_url;

                // Show success feedback
                this.showCropSuccess();
            } else {
                showAlert('Crop failed: ' + result.error, 'error');
            }
        } catch (error) {
            showAlert('Crop failed: ' + error.message, 'error');
        }
    }

    loadImage(imageUrl) {
        this.originalImageUrl = imageUrl;
        this.image.src = imageUrl;
        this.croppedImageUrl = null;
        this.resetCrop();
        this.updateContainerSize();
    }
}

// Initialize cropper when reference image is uploaded
let imageCropper = null;

function initializeReferenceCropper(imageUrl) {
    if (!imageCropper) {
        imageCropper = new ReferenceImageCropper();
    }
    imageCropper.loadImage(imageUrl);
}
```

**Integration Points**:
- **Resolution Sync**: Automatically adjust crop window when output resolution changes
- **Keyboard Shortcuts**: +/- for zoom, arrow keys for panning
- **Visual Feedback**: Rule of thirds grid, zoom level indicator
- **Original Preservation**: Keep original image for resolution changes

**Benefits of This Approach**:
- ‚úÖ **Composition Control**: Users frame subjects exactly as desired
- ‚úÖ **Aspect Ratio Matching**: Eliminates ControlNet distortion issues
- ‚úÖ **Interactive Feedback**: Real-time preview of final composition
- ‚úÖ **Resolution Independence**: Works with any output resolution
- ‚úÖ **User-Friendly**: Intuitive drag, zoom, and keyboard controls

### Phase 3: AI Integration (Week 3)
1. **z.ai GLM-4.5v Integration**
   - Implement OpenAI-compatible client
   - Add image analysis endpoint
   - Create prompt suggestion system

2. **Physical Attributes System**
   - Add attribute selection UI
   - Implement prompt enhancement logic
   - Create attribute validation

3. **Workflow Integration**
   - Connect all systems in generation pipeline
   - Add comprehensive error handling
   - Implement fallback mechanisms

### Phase 4: Testing & Optimization (Week 4) - STREAMLINED!
1. **Performance Optimization**
   - Implement image caching strategies
   - Add memory management for uploads
   - Optimize file serving and cleanup

2. **User Experience Polish**
   - Add loading states and progress bars
   - Implement better error messages
   - Add help tooltips and instructions

3. **Testing & Documentation**
   - Comprehensive testing of all workflows
   - Test reference image processing with fal.ai
   - Update user documentation

**Timeline Reduced**: Implementation now possible in **2-3 weeks** instead of 4!

## API Integration Details

### fal.ai FLUX Control LoRA Depth Configuration (SINGLE API CALL!)
```python
def create_reference_payload(prompt, loras, width, height, reference_image_url):
    payload = {
        "prompt": prompt,
        "image_url": reference_image_url,  # Reference image for depth control
        "control_lora_image_url": reference_image_url,  # Same image for depth control
        "control_lora_strength": 0.8,  # Strength of depth control
        "preprocess_depth": True,  # Automatically extract depth information
        "image_size": {"width": width, "height": height},
        "num_inference_steps": 28,
        "guidance_scale": 3.5,
        "loras": loras
    }
    return payload
```

**Key Benefits**:
- ‚úÖ **Single API call** combining LoRAs + depth control
- ‚úÖ **Automatic depth processing** with `preprocess_depth: true`
- ‚úÖ **Full LoRA support** with multiple LoRA merging
- ‚úÖ **No separate depth map generation** required

### z.ai GLM-4.5v Integration
```python
def analyze_image_with_glm(image_base64, custom_prompt=None):
    base_prompt = """Analyze this image and provide a detailed description focusing on:
    - Overall composition and style
    - Person's appearance (if present)
    - Clothing and accessories
    - Setting and background
    - Artistic style and mood

    Provide a concise but detailed description suitable for AI image generation."""

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": custom_prompt or base_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
            ]
        }
    ]

    response = requests.post(
        "https://api.z.ai/api/paas/v4/chat/completions",
        headers={
            "Authorization": f"Bearer {Z_AI_API_KEY}",
            "Content-Type": "application/json"
        },
        json={
            "model": "glm-4v-plus",
            "messages": messages,
            "max_tokens": 500
        }
    )

    return response.json()
```

## Error Handling & Fallback Strategies

### Reference Mode Failures
- **Upload Failed**: Clear reference state, show error message
- **Depth Generation Failed**: Allow manual depth map upload or disable ControlNet
- **AI Analysis Failed**: Continue with manual prompt entry
- **ControlNet Failed**: Fall back to standard generation without ControlNet

### Progressive Enhancement
- Reference mode is completely optional
- All existing functionality remains unchanged
- Reference features enhance but don't replace core workflow
- Graceful degradation when services unavailable

## Performance Considerations (DRAMATICALLY IMPROVED!)

### Reference Image Processing
- **Upload Time**: ~1-3 seconds per image (network dependent)
- **Memory Usage**: Minimal - just file I/O operations
- **File Sizes**: Reference images 1-10MB
- **Processing Time**: ‚úÖ **Zero local processing time!**

### File Management
- **Temporary Storage**: Auto-cleanup after 24 hours
- **Concurrent Processing**: No limits needed (no CPU processing!)
- **Storage Limits**: 1GB total temporary storage
- **Performance**: ‚ö° **10x faster than original plan**

## Security Considerations

### File Upload Security
- File type validation (images only)
- File size limits (10MB maximum)
- Unique filename generation to prevent conflicts
- Temporary storage in isolated directory

### API Security
- Environment variable protection for API keys
- Input validation for all endpoints
- Rate limiting on compute-intensive operations
- Proper error handling without exposing internal details

## Testing Strategy

### Unit Tests
- File upload validation
- Depth map generation accuracy
- API integration reliability
- Error handling coverage

### Integration Tests
- End-to-end reference image workflow
- ControlNet generation with various inputs
- AI prompt analysis quality
- Performance under load

### User Acceptance Testing
- Upload different image types and sizes
- Test with various physical attribute combinations
- Verify prompt enhancement quality
- Validate error recovery scenarios

## Future Enhancements

### Phase 2 Features
- **Multiple ControlNet Support**: When fal.ai adds support
- **Advanced Depth Control**: Depth map editing tools
- **Pose ControlNet**: Additional control types
- **Batch Processing**: Multiple reference images

### Performance Improvements
- **GPU Acceleration**: When available
- **Cloud Depth Processing**: External depth generation service
- **Caching System**: Smart depth map caching
- **Progressive Generation**: Preview during processing

### Phase 5: User Guide & Help System (Week 4.5)

**Rationale**: As the system has grown with reference images, smart cropping, physical attributes, and multiple AI models, users need comprehensive guidance to understand the complete workflow and available options.

#### 5.1 How-To Button & Modal System

**Frontend - Help Button Integration**
```html
<!-- Add to header or main navigation -->
<div class="fixed bottom-4 right-4 z-50">
    <button id="help-button" class="bg-blue-600 hover:bg-blue-700 text-white rounded-full p-4 shadow-lg transition-all duration-200 hover:scale-110">
        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8.228 9c.549-1.165 2.03-2 3.772-2 2.21 0 4 1.343 4 3 0 1.4-1.278 2.575-3.006 2.907-.542.104-.994.54-.994 1.093m0 3h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path>
        </svg>
    </button>
</div>

<!-- Help Modal -->
<div id="help-modal" class="fixed inset-0 bg-black bg-opacity-50 z-50 hidden">
    <div class="flex items-center justify-center min-h-screen p-4">
        <div class="bg-gray-800 rounded-lg shadow-xl max-w-4xl w-full max-h-[80vh] flex flex-col">
            <!-- Modal Header -->
            <div class="flex items-center justify-between p-6 border-b border-gray-700">
                <h2 class="text-2xl font-bold text-white">üéØ falLoRA User Guide</h2>
                <button id="close-help" class="text-gray-400 hover:text-white">
                    <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path>
                    </svg>
                </button>
            </div>

            <!-- Modal Content with Navigation -->
            <div class="flex flex-1 overflow-hidden">
                <!-- Sidebar Navigation -->
                <div class="w-64 bg-gray-900 p-4 overflow-y-auto">
                    <nav class="space-y-2">
                        <a href="#getting-started" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Getting Started</a>
                        <a href="#basic-workflow" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Basic Workflow</a>
                        <a href="#reference-images" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Reference Images</a>
                        <a href="#physical-attributes" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Physical Attributes</a>
                        <a href="#models-loras" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Models & LoRAs</a>
                        <a href="#advanced-features" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Advanced Features</a>
                        <a href="#troubleshooting" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Troubleshooting</a>
                        <a href="#tips-tricks" class="help-nav-link block px-3 py-2 text-sm text-gray-300 hover:bg-gray-700 hover:text-white rounded">Tips & Tricks</a>
                    </nav>
                </div>

                <!-- Main Content Area -->
                <div class="flex-1 p-6 overflow-y-auto">
                    <div id="help-content" class="prose prose-invert max-w-none">
                        <!-- Markdown content will be loaded here -->
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
```

**JavaScript - Help System**
```javascript
class HelpSystem {
    constructor() {
        this.helpModal = document.getElementById('help-modal');
        this.helpButton = document.getElementById('help-button');
        this.closeButton = document.getElementById('close-help');
        this.helpContent = document.getElementById('help-content');

        this.helpData = this.getHelpContent();
        this.initializeEventListeners();
    }

    initializeEventListeners() {
        this.helpButton.addEventListener('click', () => this.showHelp());
        this.closeButton.addEventListener('click', () => this.hideHelp());
        this.helpModal.addEventListener('click', (e) => {
            if (e.target === this.helpModal) this.hideHelp();
        });

        // Navigation
        document.querySelectorAll('.help-nav-link').forEach(link => {
            link.addEventListener('click', (e) => {
                e.preventDefault();
                const section = e.target.getAttribute('href').substring(1);
                this.showSection(section);

                // Update active state
                document.querySelectorAll('.help-nav-link').forEach(l => l.classList.remove('bg-gray-700', 'text-white'));
                e.target.classList.add('bg-gray-700', 'text-white');
            });
        });

        // Keyboard shortcut
        document.addEventListener('keydown', (e) => {
            if (e.key === 'F1' || (e.ctrlKey && e.key === 'h')) {
                e.preventDefault();
                this.showHelp();
            }
            if (e.key === 'Escape' && !this.helpModal.classList.contains('hidden')) {
                this.hideHelp();
            }
        });
    }

    showHelp() {
        this.helpModal.classList.remove('hidden');
        this.showSection('getting-started');
        document.body.style.overflow = 'hidden';
    }

    hideHelp() {
        this.helpModal.classList.add('hidden');
        document.body.style.overflow = 'auto';
    }

    showSection(sectionName) {
        const section = this.helpData.sections.find(s => s.id === sectionName);
        if (section) {
            this.helpContent.innerHTML = this.markdownToHtml(section.content);
        }
    }

    markdownToHtml(markdown) {
        // Simple markdown to HTML conversion
        return markdown
            .replace(/^# (.*$)/gim, '<h1 class="text-3xl font-bold text-white mb-4">$1</h1>')
            .replace(/^## (.*$)/gim, '<h2 class="text-2xl font-bold text-white mt-6 mb-3">$1</h2>')
            .replace(/^### (.*$)/gim, '<h3 class="text-xl font-bold text-white mt-4 mb-2">$1</h3>')
            .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
            .replace(/^\* (.*$)/gim, '<li class="ml-4 mb-1">‚Ä¢ $1</li>')
            .replace(/\n\n/g, '</p><p class="mb-4">')
            .replace(/^(?!<[h|l])/gm, '<p class="mb-4">')
            .replace(/<p class="mb-4"><\/p>/g, '');
    }

    getHelpContent() {
        return {
            sections: [
                {
                    id: 'getting-started',
                    content: `# Getting Started with falLoRA

Welcome to **falLoRA** - a powerful character LoRA generator that combines AI models, reference images, and custom attributes to create stunning character images.

## What is falLoRA?

falLoRA is an advanced web application that:
- **Generates character images** using state-of-the-art AI models
- **Supports custom LoRAs** from Civitai and other sources
- **Uses reference images** to guide character generation
- **Applies physical attributes** for consistent character design
- **Works with multiple AI models** including FLUX, WAN, and Qwen

## Quick Start

1. **Choose a base model** from the dropdown (FLUX recommended)
2. **Add LoRAs** (optional) for custom styles/characters
3. **Set your output resolution** and other parameters
4. **Upload a reference image** (optional) for character guidance
5. **Configure physical attributes** (optional) for specific traits
6. **Write your prompt** and click "Generate"

## System Requirements

- **Modern web browser** (Chrome, Firefox, Safari, Edge)
- **Internet connection** for AI API access
- **No local installation** - everything runs in the cloud
- **Image uploads** up to 10MB for reference images`
                },
                {
                    id: 'basic-workflow',
                    content: `# Basic Workflow

Follow this step-by-step process to generate character images with falLoRA.

## Step 1: Model Selection

Choose your base AI model from the dropdown:

- **fal-ai/flux-lora** (Recommended) - Best overall quality, great with LoRAs
- **fal-ai/flux-kontext-lora** - Specialized for detailed compositions
- **fal-ai/wan/v2.2-a14b/text-to-image/lora** - Excellent for anime/stylized characters
- **fal-ai/qwen-image** - Great for realistic character generation

## Step 2: LoRA Configuration

Add custom LoRAs to modify the base model:

1. Click **"+ Add LoRA"** to add a LoRA slot
2. **Enter LoRA path** (e.g., \`username/lora-name\`)
3. **Set weight** (0.1 to 2.0) - higher values = stronger effect
4. **Remove LoRAs** with the √ó button

**Tip**: Start with weight 1.0 and adjust up/down based on results.

## Step 3: Output Settings

Configure your image output:

- **Resolution**: Choose from preset sizes or custom dimensions
- **Number of Images**: Generate 1-4 images per request
- **Inference Steps**: 20-50 steps (higher = better quality, slower)
- **Guidance Scale**: 1-20 (higher = more faithful to prompt)

## Step 4: Prompt Engineering

Write your character description:

\`\`\`
Example prompts:
- "A beautiful young woman with long flowing hair, standing in a garden"
- "A handsome man in a business suit, confident expression, office setting"
- "A fantasy elf with pointed ears, magical aura, forest background"
\`\`\`

## Step 5: Generate and Review

1. Click **"Generate Images"**
2. Wait for processing (typically 10-30 seconds)
3. Review generated images
4. Download favorites or refine and regenerate

## Best Practices

- **Be specific** in your descriptions
- **Include setting, clothing, and mood**
- **Use reference images** for consistent characters
- **Experiment with LoRA weights** for optimal results`
                },
                {
                    id: 'reference-images',
                    content: `# Reference Images

Reference images allow you to guide the AI to generate characters with specific appearances, poses, or compositions.

## How Reference Images Work

1. **Upload Reference**: Upload an image showing your desired character
2. **Smart Cropping**: Frame the subject using the interactive crop tool
3. **AI Analysis**: Optional AI analysis generates detailed prompts
4. **ControlNet Processing**: fal.ai uses depth information for composition

## Uploading Reference Images

1. Click the reference image area or drag & drop
2. Select an image file (PNG, JPG, up to 10MB)
3. The image loads in a crop window sized to your output resolution
4. Enable "Reference Mode" toggle to activate

## Smart Cropping Interface

The smart cropping tool ensures proper aspect ratio matching:

- **Pan**: Click and drag to move the image
- **Zoom**: Use mouse wheel, +/- buttons, or keyboard shortcuts
- **Grid**: Rule of thirds overlay helps with composition
- **Apply Crop**: Creates optimized version for AI processing

**Keyboard Shortcuts**:
- \`+/-\`: Zoom in/out
- \`Arrow Keys\`: Fine positioning
- \`Reset\`: Return to original framing

## Why Cropping is Necessary

- **Aspect Ratio Matching**: Prevents distortion in final output
- **Subject Focus**: Ensures important elements are properly framed
- **Depth ControlNet**: Works best with properly composed images
- **Quality Optimization**: Eliminates scaling artifacts

## Tips for Great Reference Images

- **Use clear, high-quality images**
- **Good lighting and composition**
- **Single subject works best**
- **Avoid heavily filtered images**
- **Face should be visible if character identity is important`
                },
                {
                    id: 'physical-attributes',
                    content: `# Physical Attributes

Physical attributes allow you to specify exact characteristics for your generated characters, ensuring consistency across multiple generations.

## Available Attributes

### Skin Color/Ethnicity
- White/Caucasian
- Black/African American
- Hispanic/Latino
- Asian
- Middle Eastern
- Native American
- Pacific Islander
- Mixed Race
- Mediterranean
- South Asian

### Hair Color
- Blonde
- Brunette
- Black
- Red
- Auburn
- Gray
- White
- Colorful

### Hair Style
- Long
- Short
- Medium
- Curly
- Straight
- Wavy
- Pixie Cut
- Bob
- Ponytail
- Braided

### Eye Color
- Brown
- Blue
- Green
- Hazel
- Gray
- Amber
- Violet

## How to Use Physical Attributes

1. **Set BEFORE Analysis**: Configure attributes BEFORE clicking "Analyze Reference"
2. **Combine with Reference**: Attributes override reference image characteristics
3. **AI Integration**: Attributes are incorporated into AI analysis prompts
4. **Priority System**: User specifications take precedence over AI detection

## Attribute Override System

When you specify physical attributes:
- **AI Analysis**: Incorporates your specifications into generated prompts
- **ControlNet**: Depth information still used from reference image
- **Final Output**: Character has your specified traits with reference composition

## Best Practices

- **Be consistent** for series character generation
- **Use reference images** for pose/composition + attributes for appearance
- **Combine thoughtfully** - some attributes work better together than others
- **Experiment** to find the right combination for your vision`
                },
                {
                    id: 'models-loras',
                    content: `# Models & LoRAs

Understanding the different AI models and how to effectively use LoRAs is key to getting great results.

## Base Models

### fal-ai/flux-lora (Recommended)
**Best For**: General character generation, realistic and stylized
**Strengths**: Excellent LoRA support, consistent results
**LoRA Compatibility**: Excellent with most character LoRAs

### fal-ai/flux-kontext-lora
**Best For**: Detailed compositions, complex scenes
**Strengths**: Great for environmental storytelling
**LoRA Compatibility**: Good, but more specialized

### fal-ai/wan/v2.2-a14b/text-to-image/lora
**Best For**: Anime, stylized characters, artistic styles
**Strengths**: Excellent for anime LoRAs, artistic flexibility
**LoRA Compatibility**: Best with anime/digital art LoRAs

### fal-ai/qwen-image
**Best For**: Realistic characters, photography-style images
**Strengths**: Photorealistic output, natural lighting
**LoRA Compatibility**: Good for realistic character LoRAs

## Understanding LoRAs

**LoRA** (Low-Rank Adaptation) is a fine-tuning technique that modifies AI models to generate specific styles, characters, or concepts.

### Types of LoRAs

**Character LoRAs**: Generate specific characters or people
- Example: \`username/character-name-lora\`

**Style LoRAs**: Apply specific artistic styles
- Example: \`username/art-style-lora\`

**Clothing LoRAs**: Add specific outfits or accessories
- Example: \`username/costume-lora\`

**Concept LoRAs**: Add specific themes or concepts
- Example: \`username/theme-lora\`

## LoRA Weight Management

**Weight Range**: 0.1 to 2.0
- **0.1-0.5**: Subtle influence
- **0.8-1.2**: Balanced effect (recommended starting point)
- **1.5-2.0**: Strong influence, may override other elements

**Multiple LoRAs**:
- Add multiple LoRAs for combined effects
- Weights are relative to each other
- Too many LoRAs can conflict - start with 1-2

## Finding LoRAs

**Civitai**: The largest source of character LoRAs
- Search for your character + "LoRA"
- Check compatibility ratings
- Download trigger words and usage instructions

**LoRA Formatting**:
- Civitai format: \`username/model-name\`
- Remove special characters and version numbers
- Use exact model name from Civitai

## Advanced LoRA Tips

1. **Trigger Words**: Many LoRAs require specific trigger words in your prompt
2. **Compatibility**: Not all LoRAs work with all base models
3. **Weight Tuning**: Small adjustments (0.1-0.2) can make big differences
4. **Combination**: Experiment with different LoRA combinations`
                },
                {
                    id: 'advanced-features',
                    content: `# Advanced Features

Unlock the full potential of falLoRA with these advanced features and techniques.

## AI-Powered Prompt Analysis

**z.ai GLM-4.5v Integration** for intelligent reference image analysis:

### How It Works
1. Upload reference image
2. Click "Analyze Reference"
3. AI generates detailed character description
4. Use generated prompt as-is or modify it

### Benefits
- **Detailed Descriptions**: Captures subtle details you might miss
- **Consistent Terminology**: Uses proper photography/art terms
- **Time Saving**: Quickly generates comprehensive prompts
- **Learning Tool**: Helps improve your own prompt writing

### When to Use
- **Complex Images**: Busy scenes with many elements
- **Artistic Styles**: Understanding specific art techniques
- **Beginners**: Learning good prompt structure
- **Inspiration**: Starting point for customization

## Advanced LoRA Techniques

### LoRA Stacking
Combine multiple LoRAs for complex effects:

\`\`\`
Example Combinations:
- Character LoRA (1.0) + Style LoRA (0.8) + Clothing LoRA (0.6)
- Base Character (1.2) + Art Style (0.8) + Background Theme (0.4)
\`\`\`

### Negative Prompting
Specify what you DON'T want:

\`\`\`
Avoid: deformed, blurry, bad anatomy, extra limbs
\`\`\`

### Prompt Engineering

**Structure your prompts effectively**:

\`\`\`
[Subject] + [Appearance] + [Clothing] + [Setting] + [Mood] + [Style]

Example:
"A beautiful young woman with long brown hair, wearing a blue dress,
standing in a sunlit garden, happy expression, photorealistic style"
\`\`\`

## Social Media Optimization

### Resolution Presets
- **Instagram**: 1024x1024 (square) or 1080x1920 (story)
- **TikTok**: 1080x1920 (vertical)
- **YouTube**: 1920x1080 (horizontal)
- **Twitter**: 1200x675 (landscape)

### Style Considerations
- **Instagram**: High detail, vibrant colors
- **TikTok**: Bold, attention-grabbing
- **YouTube**: Clear, scalable compositions
- **Twitter**: Simple, readable at small sizes

## Advanced Settings

### Inference Steps
- **20-30**: Quick generation, good for experiments
- **30-40**: Balanced quality and speed (recommended)
- **40-50**: Maximum quality, longer processing

### Guidance Scale
- **1-7**: Creative, varied results
- **7-12**: Balanced (recommended)
- **12-20**: Strict prompt adherence

### Batch Generation
Generate multiple variations:
- Different seeds for variety
- Same settings for consistency
- Compare results to fine-tune`
                },
                {
                    id: 'troubleshooting',
                    content: `# Troubleshooting

Solutions to common issues and problems you might encounter.

## Generation Issues

### Problem: Images Not Generating
**Possible Causes**:
- API key issues or expired credits
- Server downtime or maintenance
- Invalid LoRA names or paths
- Network connectivity problems

**Solutions**:
1. Check API key status in settings
2. Verify LoRA names are correct
3. Try with a different base model
4. Check internet connection
5. Wait a few minutes and try again

### Problem: Poor Image Quality
**Causes and Fixes**:
- **Blurry images**: Increase inference steps to 30-40
- **Distorted faces**: Adjust LoRA weights, try different model
- **Wrong style**: Check LoRA compatibility with base model
- **Inconsistent results**: Use reference images and physical attributes

### Problem: LoRA Not Working
**Troubleshooting Steps**:
1. **Verify LoRA name**: Check exact Civitai format
2. **Check weight**: Try 0.8-1.2 range
3. **Test compatibility**: Some LoRAs only work with specific models
4. **Look for trigger words**: Check LoRA description for required terms
5. **Try different model**: Switch base model and test

## Reference Image Issues

### Problem: Reference Image Not Uploading
**Solutions**:
- Check file size (max 10MB)
- Verify file format (PNG, JPG, JPEG)
- Try different image
- Clear browser cache and retry

### Problem: Cropping Not Working
**Solutions**:
- Refresh browser page
- Check JavaScript is enabled
- Try different browser
- Ensure image loads completely

### Problem: Poor Results with Reference
**Tips**:
- Use higher quality reference images
- Ensure good lighting and clarity
- Try different framing with crop tool
- Use AI analysis for better prompts

## Performance Issues

### Problem: Slow Generation
**Optimization Tips**:
- Reduce inference steps to 20-30
- Generate fewer images per batch
- Use smaller resolutions for experiments
- Close other browser tabs

### Problem: Interface Lag
**Solutions**:
- Clear browser cache
- Disable browser extensions
- Update browser to latest version
- Try different browser

## Error Messages

### "API Key Invalid"
- Check .env file configuration
- Verify fal.ai account credits
- Regenerate API key if needed

### "LoRA Not Found"
- Verify Civitai model name format
- Check model availability on Civitai
- Try alternative LoRA

### "Image Processing Failed"
- Check reference image format and size
- Verify internet connection
- Try different reference image

## Getting Help

**Built-in Help**: Press F1 or click the help button
**Community**: Join Discord community for support
**Issues**: Report bugs through GitHub issues
**Documentation**: Check online documentation for updates`
                },
                {
                    id: 'tips-tricks',
                    content: `# Tips & Tricks

Expert tips and techniques to get the most out of falLoRA.

## Prompt Writing Mastery

### The Magic Formula
**Subject + Action + Setting + Style + Details**

\`\`\`
Good: "A beautiful young woman smiling confidently, wearing a red dress,
standing in a modern apartment at sunset, photorealistic style, detailed lighting"

Bad: "Woman dress apartment"
\`\`\`

### Power Words
Include these for better results:
- **Lighting**: cinematic, dramatic, soft, golden hour
- **Quality**: ultrarealistic, highly detailed, 8K
- **Composition**: professional photography, portrait
- **Mood**: serene, joyful, mysterious, intense

### Negative Prompts
Specify what to avoid:

\`\`\`
Negative: deformed, ugly, bad anatomy, extra limbs, blurry,
low quality, distorted face, watermark, text, signature
\`\`\`

## LoRA Optimization

### Weight Experimentation
- **Character LoRAs**: Start at 1.0, adjust ¬±0.2
- **Style LoRAs**: Usually 0.6-0.8 works best
- **Clothing**: 0.4-0.6 for subtle effects
- **Background**: 0.2-0.4 to avoid overpowering

### Combinations That Work
\`\`\`
Professional Headshot:
- Character LoRA (1.0) + Portrait Style (0.6)

Fantasy Character:
- Character LoRA (1.2) + Fantasy Style (0.8) + Costume (0.6)

Realistic Scene:
- Character LoRA (0.8) + Photorealistic (0.4)
\`\`\`

## Reference Image Secrets

### Perfect Reference Images
- **Lighting**: Even, natural lighting works best
- **Composition**: Clear subject, simple background
- **Quality**: Higher resolution = better results
- **Focus**: Sharp focus on important features

### Smart Cropping Tips
- **Rule of Thirds**: Place eyes on upper third line
- **Head Room**: Leave space above head
- **Negative Space**: Include some context/background
- **Zoom Level**: Don't over-zoom and lose quality

## Workflow Efficiency

### Keyboard Shortcuts
- **F1**: Open help guide
- **Ctrl/Cmd + Enter**: Generate images
- **Escape**: Close modals
- **+/-**: Zoom in/out (crop tool)
- **Arrow Keys**: Pan image (crop tool)

### Batch Processing
- Generate multiple images with different seeds
- Keep good settings as templates
- Document successful LoRA combinations
- Use reference images for consistency

## Quality vs Speed

### Fast Experiments
- 20 inference steps
- 1024x1024 resolution
- Single image generation
- Higher guidance scale (8-10)

### Maximum Quality
- 40-50 inference steps
- Higher resolutions
- Multiple images per batch
- Lower guidance scale (5-7)

## Advanced Techniques

### Character Consistency
1. Use same reference image across generations
2. Apply physical attributes consistently
3. Keep LoRA weights the same
4. Document successful prompt formulas

### Style Transfer
- Use style LoRAs with different characters
- Combine reference images + style LoRAs
- Experiment with weight combinations
- Keep prompts consistent for comparison

### Troubleshooting Bad Results
- **Too much LoRA?** Reduce weights
- **Not enough detail?** Increase inference steps
- **Wrong style?** Try different base model
- **Inconsistent results?** Use reference images

## Pro Tips

### Save Successful Settings
- Screenshot good parameter combinations
- Keep notes on effective LoRA pairings
- Save reference images that work well
- Document prompt structures

### Community Wisdom
- Share successful combinations
- Learn from others' examples
- Stay updated on new LoRAs
- Contribute your own findings

## Inspiration

### Prompt Ideas
- **Character Archetypes**: Hero, villain, mentor, sidekick
- **Genres**: Fantasy, sci-fi, romance, historical
- **Emotions**: Joy, sadness, anger, surprise
- **Settings**: Urban, natural, futuristic, historical

### Style Exploration
- **Photography**: Portrait, landscape, street, fashion
- **Art**: Oil painting, watercolor, anime, pixel art
- **Eras**: Victorian, cyberpunk, medieval, modern
- **Moods**: Dreamy, dramatic, peaceful, intense`
                }
            ]
        };
    }
}

// Initialize help system when DOM is loaded
document.addEventListener('DOMContentLoaded', () => {
    window.helpSystem = new HelpSystem();
});
```

#### 5.2 Integration Points

**Add to main HTML**:
```html
<!-- Add before closing body tag -->
<script src="help-system.js"></script>
```

**CSS Styling**:
```css
/* Add to style.css */
.help-nav-link.active {
    @apply bg-gray-700 text-white;
}

#help-content h1 {
    @apply text-3xl font-bold text-white mb-4;
}

#help-content h2 {
    @apply text-2xl font-bold text-white mt-6 mb-3;
}

#help-content h3 {
    @apply text-xl font-bold text-white mt-4 mb-2;
}

#help-content p {
    @apply mb-4 text-gray-300;
}

#help-content li {
    @apply ml-4 mb-1 text-gray-300;
}

#help-content code {
    @apply bg-gray-700 px-1 py-0.5 rounded text-sm text-green-400;
}

#help-content pre {
    @apply bg-gray-900 p-4 rounded-lg mb-4 overflow-x-auto;
}
```

### 5.3 User Benefits

**Comprehensive Coverage**:
- Step-by-step workflows
- Feature explanations
- Troubleshooting guide
- Expert tips and tricks

**Easy Access**:
- F1 keyboard shortcut
- Always-visible help button
- Searchable navigation
- Mobile-friendly design

**Practical Examples**:
- Real prompt examples
- LoRA combinations
- Setting recommendations
- Common solutions

## Conclusion

This **COMPREHENSIVE** implementation plan provides a complete reference image integration system with user guidance:

1. **Complete User Experience**: From basic generation to advanced features
2. **Smart Reference System**: AI-powered analysis with intelligent cropping
3. **Comprehensive Help System**: Built-in guide with examples and troubleshooting
4. **Professional Quality**: Industry-best practices and expert techniques
5. **Future-Ready**: Extensible architecture for additional features

## Key Achievements:

‚úÖ **Simplified Architecture**: 90% complexity reduction
‚úÖ **Smart Cropping**: Solves ControlNet scaling issues
‚úÖ **AI Integration**: Intelligent prompt analysis and enhancement
‚úÖ **User Guidance**: Comprehensive help system and documentation
‚úÖ **Professional Workflow**: From beginner to expert techniques

The complete system provides powerful reference image capabilities while maintaining usability for users of all skill levels. The phased approach ensures each component works reliably while building toward a comprehensive character generation platform.

### Timeline: 4.5 weeks total implementation time